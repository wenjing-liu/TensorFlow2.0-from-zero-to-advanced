{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.2\n",
      "numpy 1.17.4\n",
      "pandas 0.25.3\n",
      "sklearn 0.22\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. preprocessing data\n",
    "# 2. bulid model\n",
    "# 2.1 encoder\n",
    "# 2.2 attention\n",
    "# 2.3 decoder\n",
    "# 2.4 loss & optimizer\n",
    "# 2.5 train\n",
    "# 3. evalutation\n",
    "# 3.1 given sentence, return translated results\n",
    "# 3.2 visualize results (attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then   what.\n",
      "¿Entonces que?\n"
     ]
    }
   ],
   "source": [
    "en_spa_file_path = './data_span_en/spa.txt'\n",
    "\n",
    "import unicodedata\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "en_sentence = 'Then   what.'\n",
    "sp_sentence = '¿Entonces qué?'\n",
    "print(unicode_to_ascii(en_sentence))\n",
    "print(unicode_to_ascii(sp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> then what . <end>\n",
      "<start> ¿ entonces que ? <end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    #标点符号前后加空格\n",
    "    s = re.sub(r'([?.!,¿])', r' \\1 ', s)\n",
    "    #多余的空格变成一个空格\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    \n",
    "    # 除了标点符号和字母外都是空格\n",
    "    s = re.sub(r'[^a-zA-Z?.!,¿]', ' ', s)\n",
    "    #去掉前后空格\n",
    "    s = s.rstrip().strip()\n",
    "    \n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s\n",
    "\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "def parse_data(filename):\n",
    "    lines = open(filename, encoding='UTF-8').read().strip().split('\\n')\n",
    "    sentence_pairs = [line.split('\\t') for line in lines]\n",
    "    preprocessed_sentence_pairs = [\n",
    "        (preprocess_sentence(en), preprocess_sentence(sp)) for en, sp in sentence_pairs\n",
    "    ]\n",
    "    return zip(*preprocessed_sentence_pairs)\n",
    "\n",
    "en_dataset, sp_dataset = parse_data(en_spa_file_path)\n",
    "print(en_dataset[-1])\n",
    "print(sp_dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 8\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "        num_words=None, filters='', split=' ')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:3000])\n",
    "output_tensor, output_tokenizer = tokenizer(en_dataset[0:3000])\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_input = max_length(input_tensor)\n",
    "max_length_output = max_length(output_tensor)\n",
    "print(max_length_input, max_length_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 600, 2400, 600)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor, test_size=0.2)\n",
    "len(input_train), len(input_eval), len(output_train), len(output_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 --> <start>\n",
      "14 --> no\n",
      "17 --> se\n",
      "288 --> nadar\n",
      "3 --> .\n",
      "2 --> <end>\n",
      "\n",
      "1 --> <start>\n",
      "4 --> i\n",
      "23 --> can\n",
      "21 --> t\n",
      "246 --> swim\n",
      "3 --> .\n",
      "2 --> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(example, tokenizer):\n",
    "    for t in example:\n",
    "        if t != 0:\n",
    "            print('%d --> %s' % (t, tokenizer.index_word[t]))\n",
    "            \n",
    "convert(input_train[0], input_tokenizer)\n",
    "print()\n",
    "convert(output_train[0], output_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(30000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "train_dataset = make_dataset(input_train, output_train, batch_size, epochs, True)\n",
    "eval_dataset = make_dataset(input_eval, output_eval, batch_size, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 11) (64, 8)\n",
      "tf.Tensor(\n",
      "[[   1   14 1335    3    2    0    0    0    0    0    0]\n",
      " [   1   12  387   11  388    3    2    0    0    0    0]\n",
      " [   1  776    8    4    3    2    0    0    0    0    0]\n",
      " [   1    5   13   51  185    6    2    0    0    0    0]\n",
      " [   1    5   39 1697    6    2    0    0    0    0    0]\n",
      " [   1 1013   23  194    3    2    0    0    0    0    0]\n",
      " [   1    4   17 1031    3    2    0    0    0    0    0]\n",
      " [   1   11  342    3    2    0    0    0    0    0    0]\n",
      " [   1  596  597    3    2    0    0    0    0    0    0]\n",
      " [   1    5   93  373    6    2    0    0    0    0    0]\n",
      " [   1    9   34 1474    3    2    0    0    0    0    0]\n",
      " [   1   18   77  659    3    2    0    0    0    0    0]\n",
      " [   1   38  806    3    2    0    0    0    0    0    0]\n",
      " [   1   10  860    3    2    0    0    0    0    0    0]\n",
      " [   1  660 1411    3    2    0    0    0    0    0    0]\n",
      " [   1  892    8   29    7    2    0    0    0    0    0]\n",
      " [   1  362  980    3    2    0    0    0    0    0    0]\n",
      " [   1   16  857    3    2    0    0    0    0    0    0]\n",
      " [   1   16  291    3    2    0    0    0    0    0    0]\n",
      " [   1   57   20    3    2    0    0    0    0    0    0]\n",
      " [   1   29 1234    3    2    0    0    0    0    0    0]\n",
      " [   1  116  624    3    2    0    0    0    0    0    0]\n",
      " [   1    4    9 1597    3    2    0    0    0    0    0]\n",
      " [   1  341   44  305    3    2    0    0    0    0    0]\n",
      " [   1    4  488    3    2    0    0    0    0    0    0]\n",
      " [   1    4   12  296    3    2    0    0    0    0    0]\n",
      " [   1   18   39 1792    3    2    0    0    0    0    0]\n",
      " [   1  203  150  113   76    3    2    0    0    0    0]\n",
      " [   1   18   47   11  100    3    2    0    0    0    0]\n",
      " [   1  250  451    3    2    0    0    0    0    0    0]\n",
      " [   1  121    3    2    0    0    0    0    0    0    0]\n",
      " [   1    5   59   32   31    6    2    0    0    0    0]\n",
      " [   1   18   16   70   97    3    2    0    0    0    0]\n",
      " [   1   17   20    3    2    0    0    0    0    0    0]\n",
      " [   1   12   23  265    3    2    0    0    0    0    0]\n",
      " [   1  319 1687    3    2    0    0    0    0    0    0]\n",
      " [   1   19  565    7    2    0    0    0    0    0    0]\n",
      " [   1   10 1157    3    2    0    0    0    0    0    0]\n",
      " [   1    4  205    3    2    0    0    0    0    0    0]\n",
      " [   1   17  153  406    3    2    0    0    0    0    0]\n",
      " [   1  810    3    2    0    0    0    0    0    0    0]\n",
      " [   1   13 1163    3    2    0    0    0    0    0    0]\n",
      " [   1   38 1052    3    2    0    0    0    0    0    0]\n",
      " [   1  304  101    3    2    0    0    0    0    0    0]\n",
      " [   1    5   54   25  108   28   11 1783    6    2    0]\n",
      " [   1   10  476    3    2    0    0    0    0    0    0]\n",
      " [   1   46 1627    3    2    0    0    0    0    0    0]\n",
      " [   1    5   10  106    6    2    0    0    0    0    0]\n",
      " [   1   11  111   22  117    3    2    0    0    0    0]\n",
      " [   1   99  685    3    2    0    0    0    0    0    0]\n",
      " [   1  945    3    2    0    0    0    0    0    0    0]\n",
      " [   1  764    3    2    0    0    0    0    0    0    0]\n",
      " [   1  176  191    7    2    0    0    0    0    0    0]\n",
      " [   1   17  689    3    2    0    0    0    0    0    0]\n",
      " [   1  630   70    3    2    0    0    0    0    0    0]\n",
      " [   1   14  277  516    3    2    0    0    0    0    0]\n",
      " [   1  189    3    2    0    0    0    0    0    0    0]\n",
      " [   1    9  167    3    2    0    0    0    0    0    0]\n",
      " [   1    5   42   88    4    6    2    0    0    0    0]\n",
      " [   1  318    8   81    3    2    0    0    0    0    0]\n",
      " [   1   29 1251    3    2    0    0    0    0    0    0]\n",
      " [   1  195    7    2    0    0    0    0    0    0    0]\n",
      " [   1  539   20    3    2    0    0    0    0    0    0]\n",
      " [   1    5   27   17  881    6    2    0    0    0    0]], shape=(64, 11), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  1  27  21 686   3   2   0   0]\n",
      " [  1   4   9 206   3   2   0   0]\n",
      " [  1 320   5   3   2   0   0   0]\n",
      " [  1  30  11 126   7   2   0   0]\n",
      " [  1  23   4  58   6   7   2   0]\n",
      " [  1  63 131   3   2   0   0   0]\n",
      " [  1   5 412   3   2   0   0   0]\n",
      " [  1  13   8 180   3   2   0   0]\n",
      " [  1 101 209   3   2   0   0   0]\n",
      " [  1  23  11  20   7   2   0   0]\n",
      " [  1   6   8  17 749   3   2   0]\n",
      " [  1   4  85  11   3   2   0   0]\n",
      " [  1  14 169   3   2   0   0   0]\n",
      " [  1   4   9 135   3   2   0   0]\n",
      " [  1   4 204 711   3   2   0   0]\n",
      " [  1 235   5   3   2   0   0   0]\n",
      " [  1  45 135   3   2   0   0   0]\n",
      " [  1   4   9 332   3   2   0   0]\n",
      " [  1   4   9 207   3   2   0   0]\n",
      " [  1  59  18   3   2   0   0   0]\n",
      " [  1   5 657   3   2   0   0   0]\n",
      " [  1  14  22 124   3   2   0   0]\n",
      " [  1   5   8 792   3   2   0   0]\n",
      " [  1  59 163 214   3   2   0   0]\n",
      " [  1   5 427   3   2   0   0   0]\n",
      " [  1   5  56  16   3   2   0   0]\n",
      " [  1   4  23 419   6   3   2   0]\n",
      " [  1 125  88   3   2   0   0   0]\n",
      " [  1   4  64  95   3   2   0   0]\n",
      " [  1  69 414   3   2   0   0   0]\n",
      " [  1  15  32  26   3   2   0   0]\n",
      " [  1  30  11 171   7   2   0   0]\n",
      " [  1   4  33 697   3   2   0   0]\n",
      " [  1   4  55  18   3   2   0   0]\n",
      " [  1   4  84   6   3   2   0   0]\n",
      " [  1  31 398   3   2   0   0   0]\n",
      " [  1  50 351  10   2   0   0   0]\n",
      " [  1   4   9 637   3   2   0   0]\n",
      " [  1   5 428   3   2   0   0   0]\n",
      " [  1  13 441 222   3   2   0   0]\n",
      " [  1  67 222   3   2   0   0   0]\n",
      " [  1   6   8 640   3   2   0   0]\n",
      " [  1  14 264   3   2   0   0   0]\n",
      " [  1 165  34  26   3   2   0   0]\n",
      " [  1  50   8 315   7   2   0   0]\n",
      " [  1   4   9 133   3   2   0   0]\n",
      " [  1  14  22 795   3   2   0   0]\n",
      " [  1  33   4  81   7   2   0   0]\n",
      " [  1  13 211  17 143   3   2   0]\n",
      " [  1  19   8  70   3   2   0   0]\n",
      " [  1   4  24  44   3   2   0   0]\n",
      " [  1  38  47  10   2   0   0   0]\n",
      " [  1  14 231  80  10   2   0   0]\n",
      " [  1  11 199   3   2   0   0   0]\n",
      " [  1  31 374   3   2   0   0   0]\n",
      " [  1 240   6   3   2   0   0   0]\n",
      " [  1 166   3   2   0   0   0   0]\n",
      " [  1   6   8 194   3   2   0   0]\n",
      " [  1  73   5 370   7   2   0   0]\n",
      " [  1  91 393   3   2   0   0   0]\n",
      " [  1   5 227   3   2   0   0   0]\n",
      " [  1  38  61  10   2   0   0   0]\n",
      " [  1  59  18   3   2   0   0   0]\n",
      " [  1  35 218   7   2   0   0   0]], shape=(64, 8), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    print(x.shape, y.shape)\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_units = 256\n",
    "units = 1024\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 11, 1024) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        \n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "       \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))\n",
    "    \n",
    "encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sampel_hidden = encoder(x, sample_hidden)\n",
    "\n",
    "print(sample_output.shape, sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1024) (64, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "    \n",
    "    \n",
    "    def call(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden.shape: (batch_size, units)\n",
    "        # encoder_outputs.shape: (batch_size, length, units)\n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "        \n",
    "        # before V: (batch_size, length, units)\n",
    "        # after V: (batch_size, length, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n",
    "        \n",
    "        # shape: (batch_size, length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
    "        \n",
    "        # context_vector.shape: (batch_size, length, units)\n",
    "        context_vector = attention_weights * encoder_outputs\n",
    "        \n",
    "        # context_vector.shape: (batch_size, units)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "attention_model = BahdanauAttention(units = 10)\n",
    "attention_results, attention_weights = attention_model(sampel_hidden, sample_output)\n",
    "\n",
    "print(attention_results.shape, attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 918) (64, 1024) (64, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, decoding_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.decoding_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.decoding_units)\n",
    "    \n",
    "    \n",
    "    def call(self, x, hidden, encoding_outputs):\n",
    "        # context_vector.shape: (batch_size, units)\n",
    "        context_vector, attention_weights = self.attention(hidden, encoding_outputs)\n",
    "        \n",
    "        # before embedding: x.shape: (batch_size, 1)\n",
    "        # after embedding: x.shape: (batch_size, 1, embedding_units)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        combined_x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
    "        \n",
    "        # output.shape: (batch_size, 1, decoding_units)\n",
    "        # state.shape: (batch_size, decoding_units)\n",
    "        output, state = self.gru(combined_x)\n",
    "        \n",
    "        #output.shape: (batch_size, vocab_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output, state, attention_weights\n",
    "    \n",
    "decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n",
    "outputs = decoder(tf.random.uniform((batch_size, 1)),\n",
    "                 sampel_hidden,\n",
    "                 sample_output)\n",
    "\n",
    "decoder_ouput, decoder_hidden, decoder_aw = outputs\n",
    "\n",
    "print(decoder_ouput.shape, decoder_hidden.shape, decoder_aw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction = 'none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, encoding_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_outputs, encoding_hidden = encoder(inp, encoding_hidden)\n",
    "    \n",
    "        decoding_hidden = encoding_hidden\n",
    "    \n",
    "        # eg.: <start> I am here <end>\n",
    "        # 1. <start> -> I\n",
    "        # 2. I -> am\n",
    "        # 3. am -> here\n",
    "        # 4. here -> <end>\n",
    "        for t in range(0, targ.shape[1] - 1):\n",
    "            decoding_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "            predictions, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_outputs)\n",
    "            loss += loss_function(targ[:, t+1], predictions)\n",
    "    \n",
    "    batch_loss = loss / int(targ.shape[0])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.4913\n",
      "Epoch 1 Loss 0.0107\n",
      "Time take for 1 epoch 21.633755922317505 sec\n",
      "\n",
      "Epoch 1 Loss 0.0215\n",
      "Time take for 1 epoch 23.870332956314087 sec\n",
      "\n",
      "Epoch 1 Loss 0.0324\n",
      "Time take for 1 epoch 25.769220113754272 sec\n",
      "\n",
      "Epoch 1 Loss 0.0424\n",
      "Time take for 1 epoch 28.022496938705444 sec\n",
      "\n",
      "Epoch 1 Loss 0.0506\n",
      "Time take for 1 epoch 29.9185049533844 sec\n",
      "\n",
      "Epoch 1 Loss 0.0633\n",
      "Time take for 1 epoch 31.846547842025757 sec\n",
      "\n",
      "Epoch 1 Loss 0.0715\n",
      "Time take for 1 epoch 33.794620990753174 sec\n",
      "\n",
      "Epoch 1 Loss 0.0789\n",
      "Time take for 1 epoch 35.62268900871277 sec\n",
      "\n",
      "Epoch 1 Loss 0.0866\n",
      "Time take for 1 epoch 37.650452852249146 sec\n",
      "\n",
      "Epoch 1 Loss 0.0949\n",
      "Time take for 1 epoch 39.72517490386963 sec\n",
      "\n",
      "Epoch 1 Loss 0.1029\n",
      "Time take for 1 epoch 41.656322956085205 sec\n",
      "\n",
      "Epoch 1 Loss 0.1106\n",
      "Time take for 1 epoch 43.74341082572937 sec\n",
      "\n",
      "Epoch 1 Loss 0.1177\n",
      "Time take for 1 epoch 45.740249156951904 sec\n",
      "\n",
      "Epoch 1 Loss 0.1247\n",
      "Time take for 1 epoch 48.10359001159668 sec\n",
      "\n",
      "Epoch 1 Loss 0.1318\n",
      "Time take for 1 epoch 50.4001247882843 sec\n",
      "\n",
      "Epoch 1 Loss 0.1389\n",
      "Time take for 1 epoch 52.720458030700684 sec\n",
      "\n",
      "Epoch 1 Loss 0.1457\n",
      "Time take for 1 epoch 55.32692503929138 sec\n",
      "\n",
      "Epoch 1 Loss 0.1523\n",
      "Time take for 1 epoch 57.53177499771118 sec\n",
      "\n",
      "Epoch 1 Loss 0.1587\n",
      "Time take for 1 epoch 59.301783084869385 sec\n",
      "\n",
      "Epoch 1 Loss 0.1651\n",
      "Time take for 1 epoch 61.180009841918945 sec\n",
      "\n",
      "Epoch 1 Loss 0.1715\n",
      "Time take for 1 epoch 62.995250940322876 sec\n",
      "\n",
      "Epoch 1 Loss 0.1778\n",
      "Time take for 1 epoch 64.93137407302856 sec\n",
      "\n",
      "Epoch 1 Loss 0.1842\n",
      "Time take for 1 epoch 66.8110179901123 sec\n",
      "\n",
      "Epoch 1 Loss 0.1902\n",
      "Time take for 1 epoch 68.59645199775696 sec\n",
      "\n",
      "Epoch 1 Loss 0.1965\n",
      "Time take for 1 epoch 70.51217198371887 sec\n",
      "\n",
      "Epoch 1 Loss 0.2029\n",
      "Time take for 1 epoch 72.603266954422 sec\n",
      "\n",
      "Epoch 1 Loss 0.2090\n",
      "Time take for 1 epoch 75.2756679058075 sec\n",
      "\n",
      "Epoch 1 Loss 0.2150\n",
      "Time take for 1 epoch 77.5798819065094 sec\n",
      "\n",
      "Epoch 1 Loss 0.2210\n",
      "Time take for 1 epoch 79.63538408279419 sec\n",
      "\n",
      "Epoch 1 Loss 0.2271\n",
      "Time take for 1 epoch 81.87489414215088 sec\n",
      "\n",
      "Epoch 1 Loss 0.2327\n",
      "Time take for 1 epoch 83.81617212295532 sec\n",
      "\n",
      "Epoch 1 Loss 0.2387\n",
      "Time take for 1 epoch 85.64718413352966 sec\n",
      "\n",
      "Epoch 1 Loss 0.2448\n",
      "Time take for 1 epoch 87.41507411003113 sec\n",
      "\n",
      "Epoch 1 Loss 0.2508\n",
      "Time take for 1 epoch 89.1720380783081 sec\n",
      "\n",
      "Epoch 1 Loss 0.2567\n",
      "Time take for 1 epoch 91.08077788352966 sec\n",
      "\n",
      "Epoch 1 Loss 0.2624\n",
      "Time take for 1 epoch 92.99658584594727 sec\n",
      "\n",
      "Epoch 1 Loss 0.2680\n",
      "Time take for 1 epoch 94.78901195526123 sec\n",
      "\n",
      "Epoch 1 Loss 0.2737\n",
      "Time take for 1 epoch 96.6047101020813 sec\n",
      "\n",
      "Epoch 1 Loss 0.2791\n",
      "Time take for 1 epoch 98.35938715934753 sec\n",
      "\n",
      "Epoch 1 Loss 0.2845\n",
      "Time take for 1 epoch 100.083909034729 sec\n",
      "\n",
      "Epoch 1 Loss 0.2897\n",
      "Time take for 1 epoch 101.7998731136322 sec\n",
      "\n",
      "Epoch 1 Loss 0.2951\n",
      "Time take for 1 epoch 103.5558750629425 sec\n",
      "\n",
      "Epoch 1 Loss 0.3003\n",
      "Time take for 1 epoch 105.36987590789795 sec\n",
      "\n",
      "Epoch 1 Loss 0.3056\n",
      "Time take for 1 epoch 107.13289594650269 sec\n",
      "\n",
      "Epoch 1 Loss 0.3110\n",
      "Time take for 1 epoch 108.89573907852173 sec\n",
      "\n",
      "Epoch 1 Loss 0.3164\n",
      "Time take for 1 epoch 110.60707402229309 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2463\n",
      "Epoch 2 Loss 0.0054\n",
      "Time take for 1 epoch 2.0566132068634033 sec\n",
      "\n",
      "Epoch 2 Loss 0.0109\n",
      "Time take for 1 epoch 4.692856073379517 sec\n",
      "\n",
      "Epoch 2 Loss 0.0160\n",
      "Time take for 1 epoch 6.761476039886475 sec\n",
      "\n",
      "Epoch 2 Loss 0.0215\n",
      "Time take for 1 epoch 9.039429187774658 sec\n",
      "\n",
      "Epoch 2 Loss 0.0268\n",
      "Time take for 1 epoch 11.445610046386719 sec\n",
      "\n",
      "Epoch 2 Loss 0.0321\n",
      "Time take for 1 epoch 13.563015222549438 sec\n",
      "\n",
      "Epoch 2 Loss 0.0375\n",
      "Time take for 1 epoch 16.25224018096924 sec\n",
      "\n",
      "Epoch 2 Loss 0.0426\n",
      "Time take for 1 epoch 18.778589963912964 sec\n",
      "\n",
      "Epoch 2 Loss 0.0477\n",
      "Time take for 1 epoch 21.085326194763184 sec\n",
      "\n",
      "Epoch 2 Loss 0.0530\n",
      "Time take for 1 epoch 23.163183212280273 sec\n",
      "\n",
      "Epoch 2 Loss 0.0580\n",
      "Time take for 1 epoch 25.28977108001709 sec\n",
      "\n",
      "Epoch 2 Loss 0.0631\n",
      "Time take for 1 epoch 27.28440499305725 sec\n",
      "\n",
      "Epoch 2 Loss 0.0683\n",
      "Time take for 1 epoch 29.20521903038025 sec\n",
      "\n",
      "Epoch 2 Loss 0.0733\n",
      "Time take for 1 epoch 31.173002004623413 sec\n",
      "\n",
      "Epoch 2 Loss 0.0784\n",
      "Time take for 1 epoch 33.09348392486572 sec\n",
      "\n",
      "Epoch 2 Loss 0.0835\n",
      "Time take for 1 epoch 34.94301414489746 sec\n",
      "\n",
      "Epoch 2 Loss 0.0884\n",
      "Time take for 1 epoch 36.85339426994324 sec\n",
      "\n",
      "Epoch 2 Loss 0.0933\n",
      "Time take for 1 epoch 38.732776165008545 sec\n",
      "\n",
      "Epoch 2 Loss 0.0984\n",
      "Time take for 1 epoch 40.61127805709839 sec\n",
      "\n",
      "Epoch 2 Loss 0.1034\n",
      "Time take for 1 epoch 42.44718313217163 sec\n",
      "\n",
      "Epoch 2 Loss 0.1085\n",
      "Time take for 1 epoch 44.31166410446167 sec\n",
      "\n",
      "Epoch 2 Loss 0.1133\n",
      "Time take for 1 epoch 46.13294506072998 sec\n",
      "\n",
      "Epoch 2 Loss 0.1182\n",
      "Time take for 1 epoch 48.00744605064392 sec\n",
      "\n",
      "Epoch 2 Loss 0.1232\n",
      "Time take for 1 epoch 50.00222301483154 sec\n",
      "\n",
      "Epoch 2 Loss 0.1281\n",
      "Time take for 1 epoch 52.080886125564575 sec\n",
      "\n",
      "Epoch 2 Loss 0.1330\n",
      "Time take for 1 epoch 53.9857861995697 sec\n",
      "\n",
      "Epoch 2 Loss 0.1380\n",
      "Time take for 1 epoch 55.79902505874634 sec\n",
      "\n",
      "Epoch 2 Loss 0.1427\n",
      "Time take for 1 epoch 57.640872955322266 sec\n",
      "\n",
      "Epoch 2 Loss 0.1476\n",
      "Time take for 1 epoch 59.522343158721924 sec\n",
      "\n",
      "Epoch 2 Loss 0.1525\n",
      "Time take for 1 epoch 61.51230216026306 sec\n",
      "\n",
      "Epoch 2 Loss 0.1572\n",
      "Time take for 1 epoch 63.34764313697815 sec\n",
      "\n",
      "Epoch 2 Loss 0.1620\n",
      "Time take for 1 epoch 65.19367218017578 sec\n",
      "\n",
      "Epoch 2 Loss 0.1669\n",
      "Time take for 1 epoch 67.01898407936096 sec\n",
      "\n",
      "Epoch 2 Loss 0.1716\n",
      "Time take for 1 epoch 68.94264698028564 sec\n",
      "\n",
      "Epoch 2 Loss 0.1767\n",
      "Time take for 1 epoch 70.74345207214355 sec\n",
      "\n",
      "Epoch 2 Loss 0.1813\n",
      "Time take for 1 epoch 72.54862308502197 sec\n",
      "\n",
      "Epoch 2 Loss 0.1862\n",
      "Time take for 1 epoch 74.43299198150635 sec\n",
      "\n",
      "Epoch 2 Loss 0.1909\n",
      "Time take for 1 epoch 76.38235425949097 sec\n",
      "\n",
      "Epoch 2 Loss 0.1955\n",
      "Time take for 1 epoch 78.27839207649231 sec\n",
      "\n",
      "Epoch 2 Loss 0.2004\n",
      "Time take for 1 epoch 80.16001200675964 sec\n",
      "\n",
      "Epoch 2 Loss 0.2049\n",
      "Time take for 1 epoch 81.99846124649048 sec\n",
      "\n",
      "Epoch 2 Loss 0.2095\n",
      "Time take for 1 epoch 83.8116819858551 sec\n",
      "\n",
      "Epoch 2 Loss 0.2140\n",
      "Time take for 1 epoch 85.68005800247192 sec\n",
      "\n",
      "Epoch 2 Loss 0.2186\n",
      "Time take for 1 epoch 87.51563596725464 sec\n",
      "\n",
      "Epoch 2 Loss 0.2231\n",
      "Time take for 1 epoch 89.5194411277771 sec\n",
      "\n",
      "Epoch 2 Loss 0.2276\n",
      "Time take for 1 epoch 91.48782300949097 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2183\n",
      "Epoch 3 Loss 0.0047\n",
      "Time take for 1 epoch 1.8610529899597168 sec\n",
      "\n",
      "Epoch 3 Loss 0.0092\n",
      "Time take for 1 epoch 3.891676902770996 sec\n",
      "\n",
      "Epoch 3 Loss 0.0139\n",
      "Time take for 1 epoch 5.73725700378418 sec\n",
      "\n",
      "Epoch 3 Loss 0.0185\n",
      "Time take for 1 epoch 7.605834007263184 sec\n",
      "\n",
      "Epoch 3 Loss 0.0229\n",
      "Time take for 1 epoch 9.598091840744019 sec\n",
      "\n",
      "Epoch 3 Loss 0.0276\n",
      "Time take for 1 epoch 11.474294900894165 sec\n",
      "\n",
      "Epoch 3 Loss 0.0324\n",
      "Time take for 1 epoch 13.339370965957642 sec\n",
      "\n",
      "Epoch 3 Loss 0.0369\n",
      "Time take for 1 epoch 15.201171875 sec\n",
      "\n",
      "Epoch 3 Loss 0.0415\n",
      "Time take for 1 epoch 17.048446893692017 sec\n",
      "\n",
      "Epoch 3 Loss 0.0457\n",
      "Time take for 1 epoch 18.941298961639404 sec\n",
      "\n",
      "Epoch 3 Loss 0.0503\n",
      "Time take for 1 epoch 20.800019025802612 sec\n",
      "\n",
      "Epoch 3 Loss 0.0547\n",
      "Time take for 1 epoch 22.800758838653564 sec\n",
      "\n",
      "Epoch 3 Loss 0.0591\n",
      "Time take for 1 epoch 24.63088583946228 sec\n",
      "\n",
      "Epoch 3 Loss 0.0636\n",
      "Time take for 1 epoch 26.51914405822754 sec\n",
      "\n",
      "Epoch 3 Loss 0.0680\n",
      "Time take for 1 epoch 28.462369918823242 sec\n",
      "\n",
      "Epoch 3 Loss 0.0726\n",
      "Time take for 1 epoch 30.418429851531982 sec\n",
      "\n",
      "Epoch 3 Loss 0.0769\n",
      "Time take for 1 epoch 32.34050107002258 sec\n",
      "\n",
      "Epoch 3 Loss 0.0815\n",
      "Time take for 1 epoch 34.16683769226074 sec\n",
      "\n",
      "Epoch 3 Loss 0.0859\n",
      "Time take for 1 epoch 35.974424839019775 sec\n",
      "\n",
      "Epoch 3 Loss 0.0903\n",
      "Time take for 1 epoch 37.8821759223938 sec\n",
      "\n",
      "Epoch 3 Loss 0.0948\n",
      "Time take for 1 epoch 39.76088500022888 sec\n",
      "\n",
      "Epoch 3 Loss 0.0992\n",
      "Time take for 1 epoch 41.751192808151245 sec\n",
      "\n",
      "Epoch 3 Loss 0.1039\n",
      "Time take for 1 epoch 43.589422941207886 sec\n",
      "\n",
      "Epoch 3 Loss 0.1085\n",
      "Time take for 1 epoch 45.57573986053467 sec\n",
      "\n",
      "Epoch 3 Loss 0.1128\n",
      "Time take for 1 epoch 47.56233191490173 sec\n",
      "\n",
      "Epoch 3 Loss 0.1173\n",
      "Time take for 1 epoch 49.43691396713257 sec\n",
      "\n",
      "Epoch 3 Loss 0.1217\n",
      "Time take for 1 epoch 51.24564576148987 sec\n",
      "\n",
      "Epoch 3 Loss 0.1263\n",
      "Time take for 1 epoch 53.08473300933838 sec\n",
      "\n",
      "Epoch 3 Loss 0.1307\n",
      "Time take for 1 epoch 54.95774292945862 sec\n",
      "\n",
      "Epoch 3 Loss 0.1351\n",
      "Time take for 1 epoch 56.957687854766846 sec\n",
      "\n",
      "Epoch 3 Loss 0.1392\n",
      "Time take for 1 epoch 59.04520511627197 sec\n",
      "\n",
      "Epoch 3 Loss 0.1435\n",
      "Time take for 1 epoch 61.069819927215576 sec\n",
      "\n",
      "Epoch 3 Loss 0.1478\n",
      "Time take for 1 epoch 63.0512490272522 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 0.1522\n",
      "Time take for 1 epoch 64.94127082824707 sec\n",
      "\n",
      "Epoch 3 Loss 0.1564\n",
      "Time take for 1 epoch 66.8860878944397 sec\n",
      "\n",
      "Epoch 3 Loss 0.1607\n",
      "Time take for 1 epoch 68.69337296485901 sec\n",
      "\n",
      "Epoch 3 Loss 0.1650\n",
      "Time take for 1 epoch 70.6316590309143 sec\n",
      "\n",
      "Epoch 3 Loss 0.1692\n",
      "Time take for 1 epoch 72.68718600273132 sec\n",
      "\n",
      "Epoch 3 Loss 0.1733\n",
      "Time take for 1 epoch 74.85241889953613 sec\n",
      "\n",
      "Epoch 3 Loss 0.1772\n",
      "Time take for 1 epoch 76.96235489845276 sec\n",
      "\n",
      "Epoch 3 Loss 0.1814\n",
      "Time take for 1 epoch 79.25274085998535 sec\n",
      "\n",
      "Epoch 3 Loss 0.1856\n",
      "Time take for 1 epoch 81.29747796058655 sec\n",
      "\n",
      "Epoch 3 Loss 0.1896\n",
      "Time take for 1 epoch 83.7062418460846 sec\n",
      "\n",
      "Epoch 3 Loss 0.1938\n",
      "Time take for 1 epoch 85.71363997459412 sec\n",
      "\n",
      "Epoch 3 Loss 0.1981\n",
      "Time take for 1 epoch 87.65311074256897 sec\n",
      "\n",
      "Epoch 3 Loss 0.2023\n",
      "Time take for 1 epoch 89.72848701477051 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1835\n",
      "Epoch 4 Loss 0.0040\n",
      "Time take for 1 epoch 1.8918631076812744 sec\n",
      "\n",
      "Epoch 4 Loss 0.0081\n",
      "Time take for 1 epoch 3.739342212677002 sec\n",
      "\n",
      "Epoch 4 Loss 0.0121\n",
      "Time take for 1 epoch 5.563359022140503 sec\n",
      "\n",
      "Epoch 4 Loss 0.0162\n",
      "Time take for 1 epoch 7.46036696434021 sec\n",
      "\n",
      "Epoch 4 Loss 0.0202\n",
      "Time take for 1 epoch 9.329659938812256 sec\n",
      "\n",
      "Epoch 4 Loss 0.0242\n",
      "Time take for 1 epoch 11.176750898361206 sec\n",
      "\n",
      "Epoch 4 Loss 0.0283\n",
      "Time take for 1 epoch 13.126643896102905 sec\n",
      "\n",
      "Epoch 4 Loss 0.0322\n",
      "Time take for 1 epoch 15.005734920501709 sec\n",
      "\n",
      "Epoch 4 Loss 0.0362\n",
      "Time take for 1 epoch 16.893321990966797 sec\n",
      "\n",
      "Epoch 4 Loss 0.0403\n",
      "Time take for 1 epoch 18.862876892089844 sec\n",
      "\n",
      "Epoch 4 Loss 0.0445\n",
      "Time take for 1 epoch 20.765591144561768 sec\n",
      "\n",
      "Epoch 4 Loss 0.0484\n",
      "Time take for 1 epoch 22.7874538898468 sec\n",
      "\n",
      "Epoch 4 Loss 0.0527\n",
      "Time take for 1 epoch 24.616272926330566 sec\n",
      "\n",
      "Epoch 4 Loss 0.0567\n",
      "Time take for 1 epoch 26.57671594619751 sec\n",
      "\n",
      "Epoch 4 Loss 0.0607\n",
      "Time take for 1 epoch 28.446423053741455 sec\n",
      "\n",
      "Epoch 4 Loss 0.0647\n",
      "Time take for 1 epoch 30.40381097793579 sec\n",
      "\n",
      "Epoch 4 Loss 0.0687\n",
      "Time take for 1 epoch 32.26063585281372 sec\n",
      "\n",
      "Epoch 4 Loss 0.0730\n",
      "Time take for 1 epoch 34.21480202674866 sec\n",
      "\n",
      "Epoch 4 Loss 0.0769\n",
      "Time take for 1 epoch 36.027599811553955 sec\n",
      "\n",
      "Epoch 4 Loss 0.0809\n",
      "Time take for 1 epoch 37.97498798370361 sec\n",
      "\n",
      "Epoch 4 Loss 0.0849\n",
      "Time take for 1 epoch 39.910244941711426 sec\n",
      "\n",
      "Epoch 4 Loss 0.0888\n",
      "Time take for 1 epoch 41.811594009399414 sec\n",
      "\n",
      "Epoch 4 Loss 0.0926\n",
      "Time take for 1 epoch 43.776408195495605 sec\n",
      "\n",
      "Epoch 4 Loss 0.0967\n",
      "Time take for 1 epoch 45.63248682022095 sec\n",
      "\n",
      "Epoch 4 Loss 0.1010\n",
      "Time take for 1 epoch 47.48573899269104 sec\n",
      "\n",
      "Epoch 4 Loss 0.1052\n",
      "Time take for 1 epoch 49.43762993812561 sec\n",
      "\n",
      "Epoch 4 Loss 0.1094\n",
      "Time take for 1 epoch 51.2762508392334 sec\n",
      "\n",
      "Epoch 4 Loss 0.1133\n",
      "Time take for 1 epoch 53.11020493507385 sec\n",
      "\n",
      "Epoch 4 Loss 0.1173\n",
      "Time take for 1 epoch 54.92545413970947 sec\n",
      "\n",
      "Epoch 4 Loss 0.1213\n",
      "Time take for 1 epoch 56.79376792907715 sec\n",
      "\n",
      "Epoch 4 Loss 0.1254\n",
      "Time take for 1 epoch 58.77337694168091 sec\n",
      "\n",
      "Epoch 4 Loss 0.1296\n",
      "Time take for 1 epoch 60.74547004699707 sec\n",
      "\n",
      "Epoch 4 Loss 0.1335\n",
      "Time take for 1 epoch 62.56808614730835 sec\n",
      "\n",
      "Epoch 4 Loss 0.1374\n",
      "Time take for 1 epoch 64.52648711204529 sec\n",
      "\n",
      "Epoch 4 Loss 0.1412\n",
      "Time take for 1 epoch 66.58947992324829 sec\n",
      "\n",
      "Epoch 4 Loss 0.1453\n",
      "Time take for 1 epoch 68.97073698043823 sec\n",
      "\n",
      "Epoch 4 Loss 0.1492\n",
      "Time take for 1 epoch 70.99994897842407 sec\n",
      "\n",
      "Epoch 4 Loss 0.1529\n",
      "Time take for 1 epoch 73.45972514152527 sec\n",
      "\n",
      "Epoch 4 Loss 0.1567\n",
      "Time take for 1 epoch 75.34654498100281 sec\n",
      "\n",
      "Epoch 4 Loss 0.1603\n",
      "Time take for 1 epoch 77.17089891433716 sec\n",
      "\n",
      "Epoch 4 Loss 0.1640\n",
      "Time take for 1 epoch 79.03073596954346 sec\n",
      "\n",
      "Epoch 4 Loss 0.1677\n",
      "Time take for 1 epoch 80.86111116409302 sec\n",
      "\n",
      "Epoch 4 Loss 0.1712\n",
      "Time take for 1 epoch 83.05820178985596 sec\n",
      "\n",
      "Epoch 4 Loss 0.1749\n",
      "Time take for 1 epoch 84.97106575965881 sec\n",
      "\n",
      "Epoch 4 Loss 0.1786\n",
      "Time take for 1 epoch 86.81963181495667 sec\n",
      "\n",
      "Epoch 4 Loss 0.1822\n",
      "Time take for 1 epoch 88.73525309562683 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1631\n",
      "Epoch 5 Loss 0.0035\n",
      "Time take for 1 epoch 2.061922073364258 sec\n",
      "\n",
      "Epoch 5 Loss 0.0071\n",
      "Time take for 1 epoch 3.956058979034424 sec\n",
      "\n",
      "Epoch 5 Loss 0.0107\n",
      "Time take for 1 epoch 5.793131113052368 sec\n",
      "\n",
      "Epoch 5 Loss 0.0141\n",
      "Time take for 1 epoch 7.713175058364868 sec\n",
      "\n",
      "Epoch 5 Loss 0.0178\n",
      "Time take for 1 epoch 9.682178974151611 sec\n",
      "\n",
      "Epoch 5 Loss 0.0215\n",
      "Time take for 1 epoch 11.877702951431274 sec\n",
      "\n",
      "Epoch 5 Loss 0.0251\n",
      "Time take for 1 epoch 14.199876070022583 sec\n",
      "\n",
      "Epoch 5 Loss 0.0286\n",
      "Time take for 1 epoch 15.931862115859985 sec\n",
      "\n",
      "Epoch 5 Loss 0.0323\n",
      "Time take for 1 epoch 17.828954219818115 sec\n",
      "\n",
      "Epoch 5 Loss 0.0358\n",
      "Time take for 1 epoch 19.677103996276855 sec\n",
      "\n",
      "Epoch 5 Loss 0.0395\n",
      "Time take for 1 epoch 21.710016012191772 sec\n",
      "\n",
      "Epoch 5 Loss 0.0429\n",
      "Time take for 1 epoch 23.63203191757202 sec\n",
      "\n",
      "Epoch 5 Loss 0.0464\n",
      "Time take for 1 epoch 25.578408002853394 sec\n",
      "\n",
      "Epoch 5 Loss 0.0498\n",
      "Time take for 1 epoch 27.378793954849243 sec\n",
      "\n",
      "Epoch 5 Loss 0.0531\n",
      "Time take for 1 epoch 29.21221089363098 sec\n",
      "\n",
      "Epoch 5 Loss 0.0566\n",
      "Time take for 1 epoch 31.200152158737183 sec\n",
      "\n",
      "Epoch 5 Loss 0.0600\n",
      "Time take for 1 epoch 33.17013192176819 sec\n",
      "\n",
      "Epoch 5 Loss 0.0636\n",
      "Time take for 1 epoch 35.04147696495056 sec\n",
      "\n",
      "Epoch 5 Loss 0.0670\n",
      "Time take for 1 epoch 36.97713303565979 sec\n",
      "\n",
      "Epoch 5 Loss 0.0706\n",
      "Time take for 1 epoch 38.96585011482239 sec\n",
      "\n",
      "Epoch 5 Loss 0.0741\n",
      "Time take for 1 epoch 40.975640058517456 sec\n",
      "\n",
      "Epoch 5 Loss 0.0776\n",
      "Time take for 1 epoch 43.03242301940918 sec\n",
      "\n",
      "Epoch 5 Loss 0.0811\n",
      "Time take for 1 epoch 44.87907600402832 sec\n",
      "\n",
      "Epoch 5 Loss 0.0847\n",
      "Time take for 1 epoch 46.825217962265015 sec\n",
      "\n",
      "Epoch 5 Loss 0.0882\n",
      "Time take for 1 epoch 48.71198582649231 sec\n",
      "\n",
      "Epoch 5 Loss 0.0919\n",
      "Time take for 1 epoch 50.51036810874939 sec\n",
      "\n",
      "Epoch 5 Loss 0.0954\n",
      "Time take for 1 epoch 52.63920593261719 sec\n",
      "\n",
      "Epoch 5 Loss 0.0990\n",
      "Time take for 1 epoch 54.5765380859375 sec\n",
      "\n",
      "Epoch 5 Loss 0.1023\n",
      "Time take for 1 epoch 56.43813514709473 sec\n",
      "\n",
      "Epoch 5 Loss 0.1058\n",
      "Time take for 1 epoch 58.295814990997314 sec\n",
      "\n",
      "Epoch 5 Loss 0.1092\n",
      "Time take for 1 epoch 60.03618884086609 sec\n",
      "\n",
      "Epoch 5 Loss 0.1125\n",
      "Time take for 1 epoch 62.071573972702026 sec\n",
      "\n",
      "Epoch 5 Loss 0.1159\n",
      "Time take for 1 epoch 64.08010792732239 sec\n",
      "\n",
      "Epoch 5 Loss 0.1191\n",
      "Time take for 1 epoch 65.96037793159485 sec\n",
      "\n",
      "Epoch 5 Loss 0.1228\n",
      "Time take for 1 epoch 67.67724895477295 sec\n",
      "\n",
      "Epoch 5 Loss 0.1262\n",
      "Time take for 1 epoch 69.50438499450684 sec\n",
      "\n",
      "Epoch 5 Loss 0.1295\n",
      "Time take for 1 epoch 71.30404996871948 sec\n",
      "\n",
      "Epoch 5 Loss 0.1331\n",
      "Time take for 1 epoch 73.2002580165863 sec\n",
      "\n",
      "Epoch 5 Loss 0.1361\n",
      "Time take for 1 epoch 75.1589949131012 sec\n",
      "\n",
      "Epoch 5 Loss 0.1393\n",
      "Time take for 1 epoch 77.09696102142334 sec\n",
      "\n",
      "Epoch 5 Loss 0.1424\n",
      "Time take for 1 epoch 79.02484512329102 sec\n",
      "\n",
      "Epoch 5 Loss 0.1455\n",
      "Time take for 1 epoch 80.86863493919373 sec\n",
      "\n",
      "Epoch 5 Loss 0.1485\n",
      "Time take for 1 epoch 82.70699191093445 sec\n",
      "\n",
      "Epoch 5 Loss 0.1518\n",
      "Time take for 1 epoch 84.71805214881897 sec\n",
      "\n",
      "Epoch 5 Loss 0.1550\n",
      "Time take for 1 epoch 86.58639788627625 sec\n",
      "\n",
      "Epoch 5 Loss 0.1582\n",
      "Time take for 1 epoch 88.50930213928223 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1420\n",
      "Epoch 6 Loss 0.0031\n",
      "Time take for 1 epoch 1.9460270404815674 sec\n",
      "\n",
      "Epoch 6 Loss 0.0060\n",
      "Time take for 1 epoch 3.8916280269622803 sec\n",
      "\n",
      "Epoch 6 Loss 0.0091\n",
      "Time take for 1 epoch 5.796718120574951 sec\n",
      "\n",
      "Epoch 6 Loss 0.0121\n",
      "Time take for 1 epoch 7.863477945327759 sec\n",
      "\n",
      "Epoch 6 Loss 0.0153\n",
      "Time take for 1 epoch 9.78988790512085 sec\n",
      "\n",
      "Epoch 6 Loss 0.0184\n",
      "Time take for 1 epoch 11.701066017150879 sec\n",
      "\n",
      "Epoch 6 Loss 0.0215\n",
      "Time take for 1 epoch 13.658700942993164 sec\n",
      "\n",
      "Epoch 6 Loss 0.0243\n",
      "Time take for 1 epoch 15.605268955230713 sec\n",
      "\n",
      "Epoch 6 Loss 0.0273\n",
      "Time take for 1 epoch 17.352565050125122 sec\n",
      "\n",
      "Epoch 6 Loss 0.0304\n",
      "Time take for 1 epoch 19.102442026138306 sec\n",
      "\n",
      "Epoch 6 Loss 0.0338\n",
      "Time take for 1 epoch 20.82513999938965 sec\n",
      "\n",
      "Epoch 6 Loss 0.0371\n",
      "Time take for 1 epoch 22.581218004226685 sec\n",
      "\n",
      "Epoch 6 Loss 0.0401\n",
      "Time take for 1 epoch 24.416722059249878 sec\n",
      "\n",
      "Epoch 6 Loss 0.0431\n",
      "Time take for 1 epoch 26.3745698928833 sec\n",
      "\n",
      "Epoch 6 Loss 0.0465\n",
      "Time take for 1 epoch 28.250522136688232 sec\n",
      "\n",
      "Epoch 6 Loss 0.0496\n",
      "Time take for 1 epoch 30.10189199447632 sec\n",
      "\n",
      "Epoch 6 Loss 0.0527\n",
      "Time take for 1 epoch 31.995901107788086 sec\n",
      "\n",
      "Epoch 6 Loss 0.0558\n",
      "Time take for 1 epoch -15.14192795753479 sec\n",
      "\n",
      "Epoch 6 Loss 0.0589\n",
      "Time take for 1 epoch -13.137212991714478 sec\n",
      "\n",
      "Epoch 6 Loss 0.0620\n",
      "Time take for 1 epoch -10.680299043655396 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss 0.0653\n",
      "Time take for 1 epoch -8.590319871902466 sec\n",
      "\n",
      "Epoch 6 Loss 0.0684\n",
      "Time take for 1 epoch -6.657572984695435 sec\n",
      "\n",
      "Epoch 6 Loss 0.0715\n",
      "Time take for 1 epoch -4.838241100311279 sec\n",
      "\n",
      "Epoch 6 Loss 0.0748\n",
      "Time take for 1 epoch -2.990694999694824 sec\n",
      "\n",
      "Epoch 6 Loss 0.0779\n",
      "Time take for 1 epoch -1.0408358573913574 sec\n",
      "\n",
      "Epoch 6 Loss 0.0809\n",
      "Time take for 1 epoch 0.9716479778289795 sec\n",
      "\n",
      "Epoch 6 Loss 0.0840\n",
      "Time take for 1 epoch 3.0836589336395264 sec\n",
      "\n",
      "Epoch 6 Loss 0.0868\n",
      "Time take for 1 epoch 4.9910829067230225 sec\n",
      "\n",
      "Epoch 6 Loss 0.0899\n",
      "Time take for 1 epoch 6.896700143814087 sec\n",
      "\n",
      "Epoch 6 Loss 0.0929\n",
      "Time take for 1 epoch 8.844871044158936 sec\n",
      "\n",
      "Epoch 6 Loss 0.0959\n",
      "Time take for 1 epoch 10.81589126586914 sec\n",
      "\n",
      "Epoch 6 Loss 0.0990\n",
      "Time take for 1 epoch 12.682139158248901 sec\n",
      "\n",
      "Epoch 6 Loss 0.1023\n",
      "Time take for 1 epoch 14.71862506866455 sec\n",
      "\n",
      "Epoch 6 Loss 0.1054\n",
      "Time take for 1 epoch 16.548646211624146 sec\n",
      "\n",
      "Epoch 6 Loss 0.1084\n",
      "Time take for 1 epoch 18.688374042510986 sec\n",
      "\n",
      "Epoch 6 Loss 0.1114\n",
      "Time take for 1 epoch 20.733165979385376 sec\n",
      "\n",
      "Epoch 6 Loss 0.1143\n",
      "Time take for 1 epoch 22.77760601043701 sec\n",
      "\n",
      "Epoch 6 Loss 0.1173\n",
      "Time take for 1 epoch 24.7656831741333 sec\n",
      "\n",
      "Epoch 6 Loss 0.1199\n",
      "Time take for 1 epoch 26.656596899032593 sec\n",
      "\n",
      "Epoch 6 Loss 0.1226\n",
      "Time take for 1 epoch 28.603080987930298 sec\n",
      "\n",
      "Epoch 6 Loss 0.1254\n",
      "Time take for 1 epoch 30.557393074035645 sec\n",
      "\n",
      "Epoch 6 Loss 0.1282\n",
      "Time take for 1 epoch 32.420851945877075 sec\n",
      "\n",
      "Epoch 6 Loss 0.1309\n",
      "Time take for 1 epoch 34.787837266922 sec\n",
      "\n",
      "Epoch 6 Loss 0.1335\n",
      "Time take for 1 epoch 37.58557415008545 sec\n",
      "\n",
      "Epoch 6 Loss 0.1364\n",
      "Time take for 1 epoch 39.94029903411865 sec\n",
      "\n",
      "Epoch 6 Loss 0.1390\n",
      "Time take for 1 epoch 42.253740072250366 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1212\n",
      "Epoch 7 Loss 0.0026\n",
      "Time take for 1 epoch 2.145508289337158 sec\n",
      "\n",
      "Epoch 7 Loss 0.0054\n",
      "Time take for 1 epoch 4.91100811958313 sec\n",
      "\n",
      "Epoch 7 Loss 0.0081\n",
      "Time take for 1 epoch 7.498233079910278 sec\n",
      "\n",
      "Epoch 7 Loss 0.0109\n",
      "Time take for 1 epoch 9.484681129455566 sec\n",
      "\n",
      "Epoch 7 Loss 0.0136\n",
      "Time take for 1 epoch 11.551314115524292 sec\n",
      "\n",
      "Epoch 7 Loss 0.0166\n",
      "Time take for 1 epoch 14.832488059997559 sec\n",
      "\n",
      "Epoch 7 Loss 0.0193\n",
      "Time take for 1 epoch 17.88882803916931 sec\n",
      "\n",
      "Epoch 7 Loss 0.0219\n",
      "Time take for 1 epoch 21.53060007095337 sec\n",
      "\n",
      "Epoch 7 Loss 0.0246\n",
      "Time take for 1 epoch 25.11030626296997 sec\n",
      "\n",
      "Epoch 7 Loss 0.0273\n",
      "Time take for 1 epoch 28.172667980194092 sec\n",
      "\n",
      "Epoch 7 Loss 0.0300\n",
      "Time take for 1 epoch 30.876644134521484 sec\n",
      "\n",
      "Epoch 7 Loss 0.0329\n",
      "Time take for 1 epoch 33.21826529502869 sec\n",
      "\n",
      "Epoch 7 Loss 0.0355\n",
      "Time take for 1 epoch 35.31982207298279 sec\n",
      "\n",
      "Epoch 7 Loss 0.0382\n",
      "Time take for 1 epoch 37.38409209251404 sec\n",
      "\n",
      "Epoch 7 Loss 0.0410\n",
      "Time take for 1 epoch 39.54721999168396 sec\n",
      "\n",
      "Epoch 7 Loss 0.0435\n",
      "Time take for 1 epoch 41.45537996292114 sec\n",
      "\n",
      "Epoch 7 Loss 0.0462\n",
      "Time take for 1 epoch 43.23796510696411 sec\n",
      "\n",
      "Epoch 7 Loss 0.0489\n",
      "Time take for 1 epoch 45.14244508743286 sec\n",
      "\n",
      "Epoch 7 Loss 0.0520\n",
      "Time take for 1 epoch 47.10157918930054 sec\n",
      "\n",
      "Epoch 7 Loss 0.0548\n",
      "Time take for 1 epoch 49.01924014091492 sec\n",
      "\n",
      "Epoch 7 Loss 0.0575\n",
      "Time take for 1 epoch 50.76719307899475 sec\n",
      "\n",
      "Epoch 7 Loss 0.0602\n",
      "Time take for 1 epoch 52.47657608985901 sec\n",
      "\n",
      "Epoch 7 Loss 0.0631\n",
      "Time take for 1 epoch 54.39178395271301 sec\n",
      "\n",
      "Epoch 7 Loss 0.0658\n",
      "Time take for 1 epoch 56.216289043426514 sec\n",
      "\n",
      "Epoch 7 Loss 0.0684\n",
      "Time take for 1 epoch 58.02732801437378 sec\n",
      "\n",
      "Epoch 7 Loss 0.0711\n",
      "Time take for 1 epoch 59.79611611366272 sec\n",
      "\n",
      "Epoch 7 Loss 0.0737\n",
      "Time take for 1 epoch 62.34383702278137 sec\n",
      "\n",
      "Epoch 7 Loss 0.0761\n",
      "Time take for 1 epoch 64.49094200134277 sec\n",
      "\n",
      "Epoch 7 Loss 0.0788\n",
      "Time take for 1 epoch 66.45632004737854 sec\n",
      "\n",
      "Epoch 7 Loss 0.0816\n",
      "Time take for 1 epoch 68.16703796386719 sec\n",
      "\n",
      "Epoch 7 Loss 0.0842\n",
      "Time take for 1 epoch 69.87765312194824 sec\n",
      "\n",
      "Epoch 7 Loss 0.0872\n",
      "Time take for 1 epoch 71.97545599937439 sec\n",
      "\n",
      "Epoch 7 Loss 0.0899\n",
      "Time take for 1 epoch 74.26469421386719 sec\n",
      "\n",
      "Epoch 7 Loss 0.0926\n",
      "Time take for 1 epoch 76.19141626358032 sec\n",
      "\n",
      "Epoch 7 Loss 0.0954\n",
      "Time take for 1 epoch 78.19332003593445 sec\n",
      "\n",
      "Epoch 7 Loss 0.0982\n",
      "Time take for 1 epoch 80.28937005996704 sec\n",
      "\n",
      "Epoch 7 Loss 0.1010\n",
      "Time take for 1 epoch 82.15383315086365 sec\n",
      "\n",
      "Epoch 7 Loss 0.1037\n",
      "Time take for 1 epoch 83.93926501274109 sec\n",
      "\n",
      "Epoch 7 Loss 0.1061\n",
      "Time take for 1 epoch 86.55491900444031 sec\n",
      "\n",
      "Epoch 7 Loss 0.1083\n",
      "Time take for 1 epoch 89.3313820362091 sec\n",
      "\n",
      "Epoch 7 Loss 0.1108\n",
      "Time take for 1 epoch 92.12098908424377 sec\n",
      "\n",
      "Epoch 7 Loss 0.1130\n",
      "Time take for 1 epoch 94.4331841468811 sec\n",
      "\n",
      "Epoch 7 Loss 0.1156\n",
      "Time take for 1 epoch 96.85967898368835 sec\n",
      "\n",
      "Epoch 7 Loss 0.1181\n",
      "Time take for 1 epoch 99.53710222244263 sec\n",
      "\n",
      "Epoch 7 Loss 0.1206\n",
      "Time take for 1 epoch 101.73992919921875 sec\n",
      "\n",
      "Epoch 7 Loss 0.1231\n",
      "Time take for 1 epoch 104.24930810928345 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.1108\n",
      "Epoch 8 Loss 0.0024\n",
      "Time take for 1 epoch 2.1028800010681152 sec\n",
      "\n",
      "Epoch 8 Loss 0.0047\n",
      "Time take for 1 epoch 4.48127818107605 sec\n",
      "\n",
      "Epoch 8 Loss 0.0069\n",
      "Time take for 1 epoch 6.293374300003052 sec\n",
      "\n",
      "Epoch 8 Loss 0.0094\n",
      "Time take for 1 epoch 8.10652208328247 sec\n",
      "\n",
      "Epoch 8 Loss 0.0118\n",
      "Time take for 1 epoch 10.012115240097046 sec\n",
      "\n",
      "Epoch 8 Loss 0.0142\n",
      "Time take for 1 epoch 12.101182222366333 sec\n",
      "\n",
      "Epoch 8 Loss 0.0167\n",
      "Time take for 1 epoch 14.213160276412964 sec\n",
      "\n",
      "Epoch 8 Loss 0.0189\n",
      "Time take for 1 epoch 16.081641912460327 sec\n",
      "\n",
      "Epoch 8 Loss 0.0215\n",
      "Time take for 1 epoch 18.004571199417114 sec\n",
      "\n",
      "Epoch 8 Loss 0.0238\n",
      "Time take for 1 epoch 19.934887170791626 sec\n",
      "\n",
      "Epoch 8 Loss 0.0263\n",
      "Time take for 1 epoch 21.823956966400146 sec\n",
      "\n",
      "Epoch 8 Loss 0.0287\n",
      "Time take for 1 epoch 23.8010311126709 sec\n",
      "\n",
      "Epoch 8 Loss 0.0308\n",
      "Time take for 1 epoch 25.70652413368225 sec\n",
      "\n",
      "Epoch 8 Loss 0.0331\n",
      "Time take for 1 epoch 27.876596927642822 sec\n",
      "\n",
      "Epoch 8 Loss 0.0355\n",
      "Time take for 1 epoch 29.756401300430298 sec\n",
      "\n",
      "Epoch 8 Loss 0.0381\n",
      "Time take for 1 epoch 31.682875156402588 sec\n",
      "\n",
      "Epoch 8 Loss 0.0407\n",
      "Time take for 1 epoch 33.449881076812744 sec\n",
      "\n",
      "Epoch 8 Loss 0.0432\n",
      "Time take for 1 epoch 35.25776505470276 sec\n",
      "\n",
      "Epoch 8 Loss 0.0456\n",
      "Time take for 1 epoch 37.119768142700195 sec\n",
      "\n",
      "Epoch 8 Loss 0.0480\n",
      "Time take for 1 epoch 39.214091062545776 sec\n",
      "\n",
      "Epoch 8 Loss 0.0508\n",
      "Time take for 1 epoch 41.105332136154175 sec\n",
      "\n",
      "Epoch 8 Loss 0.0531\n",
      "Time take for 1 epoch 43.00776696205139 sec\n",
      "\n",
      "Epoch 8 Loss 0.0556\n",
      "Time take for 1 epoch 44.80399417877197 sec\n",
      "\n",
      "Epoch 8 Loss 0.0582\n",
      "Time take for 1 epoch 46.89959907531738 sec\n",
      "\n",
      "Epoch 8 Loss 0.0605\n",
      "Time take for 1 epoch 48.83270502090454 sec\n",
      "\n",
      "Epoch 8 Loss 0.0630\n",
      "Time take for 1 epoch 50.81826305389404 sec\n",
      "\n",
      "Epoch 8 Loss 0.0654\n",
      "Time take for 1 epoch 52.6425621509552 sec\n",
      "\n",
      "Epoch 8 Loss 0.0678\n",
      "Time take for 1 epoch 54.480287313461304 sec\n",
      "\n",
      "Epoch 8 Loss 0.0701\n",
      "Time take for 1 epoch 56.40599322319031 sec\n",
      "\n",
      "Epoch 8 Loss 0.0726\n",
      "Time take for 1 epoch 58.37335228919983 sec\n",
      "\n",
      "Epoch 8 Loss 0.0749\n",
      "Time take for 1 epoch 60.22568607330322 sec\n",
      "\n",
      "Epoch 8 Loss 0.0772\n",
      "Time take for 1 epoch 62.20897722244263 sec\n",
      "\n",
      "Epoch 8 Loss 0.0798\n",
      "Time take for 1 epoch 64.07255220413208 sec\n",
      "\n",
      "Epoch 8 Loss 0.0824\n",
      "Time take for 1 epoch 65.92229223251343 sec\n",
      "\n",
      "Epoch 8 Loss 0.0848\n",
      "Time take for 1 epoch 67.80425429344177 sec\n",
      "\n",
      "Epoch 8 Loss 0.0873\n",
      "Time take for 1 epoch 69.81170201301575 sec\n",
      "\n",
      "Epoch 8 Loss 0.0898\n",
      "Time take for 1 epoch 71.75891828536987 sec\n",
      "\n",
      "Epoch 8 Loss 0.0921\n",
      "Time take for 1 epoch 73.65536093711853 sec\n",
      "\n",
      "Epoch 8 Loss 0.0945\n",
      "Time take for 1 epoch 75.38770914077759 sec\n",
      "\n",
      "Epoch 8 Loss 0.0965\n",
      "Time take for 1 epoch 77.27548313140869 sec\n",
      "\n",
      "Epoch 8 Loss 0.0988\n",
      "Time take for 1 epoch 79.28821611404419 sec\n",
      "\n",
      "Epoch 8 Loss 0.1008\n",
      "Time take for 1 epoch 81.27293729782104 sec\n",
      "\n",
      "Epoch 8 Loss 0.1030\n",
      "Time take for 1 epoch 83.16622734069824 sec\n",
      "\n",
      "Epoch 8 Loss 0.1051\n",
      "Time take for 1 epoch 85.15964412689209 sec\n",
      "\n",
      "Epoch 8 Loss 0.1073\n",
      "Time take for 1 epoch 87.07836723327637 sec\n",
      "\n",
      "Epoch 8 Loss 0.1095\n",
      "Time take for 1 epoch 88.97657108306885 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0879\n",
      "Epoch 9 Loss 0.0019\n",
      "Time take for 1 epoch 1.94228196144104 sec\n",
      "\n",
      "Epoch 9 Loss 0.0042\n",
      "Time take for 1 epoch 3.8748271465301514 sec\n",
      "\n",
      "Epoch 9 Loss 0.0064\n",
      "Time take for 1 epoch 5.796937942504883 sec\n",
      "\n",
      "Epoch 9 Loss 0.0085\n",
      "Time take for 1 epoch 7.735979080200195 sec\n",
      "\n",
      "Epoch 9 Loss 0.0107\n",
      "Time take for 1 epoch 9.95918607711792 sec\n",
      "\n",
      "Epoch 9 Loss 0.0129\n",
      "Time take for 1 epoch 11.829993963241577 sec\n",
      "\n",
      "Epoch 9 Loss 0.0151\n",
      "Time take for 1 epoch 13.618272066116333 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss 0.0171\n",
      "Time take for 1 epoch 15.581945896148682 sec\n",
      "\n",
      "Epoch 9 Loss 0.0192\n",
      "Time take for 1 epoch 17.480068922042847 sec\n",
      "\n",
      "Epoch 9 Loss 0.0212\n",
      "Time take for 1 epoch 19.404552936553955 sec\n",
      "\n",
      "Epoch 9 Loss 0.0233\n",
      "Time take for 1 epoch 21.292951822280884 sec\n",
      "\n",
      "Epoch 9 Loss 0.0252\n",
      "Time take for 1 epoch 23.078701972961426 sec\n",
      "\n",
      "Epoch 9 Loss 0.0273\n",
      "Time take for 1 epoch 24.880599975585938 sec\n",
      "\n",
      "Epoch 9 Loss 0.0294\n",
      "Time take for 1 epoch 26.7923321723938 sec\n",
      "\n",
      "Epoch 9 Loss 0.0318\n",
      "Time take for 1 epoch 28.694087028503418 sec\n",
      "\n",
      "Epoch 9 Loss 0.0339\n",
      "Time take for 1 epoch 30.47640085220337 sec\n",
      "\n",
      "Epoch 9 Loss 0.0360\n",
      "Time take for 1 epoch 32.35990786552429 sec\n",
      "\n",
      "Epoch 9 Loss 0.0381\n",
      "Time take for 1 epoch 34.42905402183533 sec\n",
      "\n",
      "Epoch 9 Loss 0.0404\n",
      "Time take for 1 epoch 36.40695786476135 sec\n",
      "\n",
      "Epoch 9 Loss 0.0426\n",
      "Time take for 1 epoch 38.330815076828 sec\n",
      "\n",
      "Epoch 9 Loss 0.0448\n",
      "Time take for 1 epoch 40.154792070388794 sec\n",
      "\n",
      "Epoch 9 Loss 0.0470\n",
      "Time take for 1 epoch 42.109883069992065 sec\n",
      "\n",
      "Epoch 9 Loss 0.0492\n",
      "Time take for 1 epoch 43.909729957580566 sec\n",
      "\n",
      "Epoch 9 Loss 0.0513\n",
      "Time take for 1 epoch 45.74476909637451 sec\n",
      "\n",
      "Epoch 9 Loss 0.0537\n",
      "Time take for 1 epoch 47.52971887588501 sec\n",
      "\n",
      "Epoch 9 Loss 0.0560\n",
      "Time take for 1 epoch 49.46820688247681 sec\n",
      "\n",
      "Epoch 9 Loss 0.0583\n",
      "Time take for 1 epoch 51.4248321056366 sec\n",
      "\n",
      "Epoch 9 Loss 0.0607\n",
      "Time take for 1 epoch 53.22541904449463 sec\n",
      "\n",
      "Epoch 9 Loss 0.0629\n",
      "Time take for 1 epoch 55.219155073165894 sec\n",
      "\n",
      "Epoch 9 Loss 0.0652\n",
      "Time take for 1 epoch 57.093437910079956 sec\n",
      "\n",
      "Epoch 9 Loss 0.0676\n",
      "Time take for 1 epoch 59.14569306373596 sec\n",
      "\n",
      "Epoch 9 Loss 0.0698\n",
      "Time take for 1 epoch 60.989479064941406 sec\n",
      "\n",
      "Epoch 9 Loss 0.0722\n",
      "Time take for 1 epoch 62.864787101745605 sec\n",
      "\n",
      "Epoch 9 Loss 0.0745\n",
      "Time take for 1 epoch 64.7687349319458 sec\n",
      "\n",
      "Epoch 9 Loss 0.0767\n",
      "Time take for 1 epoch 66.63986802101135 sec\n",
      "\n",
      "Epoch 9 Loss 0.0787\n",
      "Time take for 1 epoch 68.73691296577454 sec\n",
      "\n",
      "Epoch 9 Loss 0.0810\n",
      "Time take for 1 epoch 70.52663993835449 sec\n",
      "\n",
      "Epoch 9 Loss 0.0831\n",
      "Time take for 1 epoch 72.60198402404785 sec\n",
      "\n",
      "Epoch 9 Loss 0.0850\n",
      "Time take for 1 epoch 74.51836705207825 sec\n",
      "\n",
      "Epoch 9 Loss 0.0868\n",
      "Time take for 1 epoch 76.50763392448425 sec\n",
      "\n",
      "Epoch 9 Loss 0.0890\n",
      "Time take for 1 epoch 78.4439480304718 sec\n",
      "\n",
      "Epoch 9 Loss 0.0909\n",
      "Time take for 1 epoch 80.33899402618408 sec\n",
      "\n",
      "Epoch 9 Loss 0.0929\n",
      "Time take for 1 epoch 82.27992677688599 sec\n",
      "\n",
      "Epoch 9 Loss 0.0949\n",
      "Time take for 1 epoch 84.17032194137573 sec\n",
      "\n",
      "Epoch 9 Loss 0.0968\n",
      "Time take for 1 epoch 86.16192388534546 sec\n",
      "\n",
      "Epoch 9 Loss 0.0989\n",
      "Time take for 1 epoch 87.96373200416565 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0817\n",
      "Epoch 10 Loss 0.0018\n",
      "Time take for 1 epoch 1.7841908931732178 sec\n",
      "\n",
      "Epoch 10 Loss 0.0038\n",
      "Time take for 1 epoch 3.7601330280303955 sec\n",
      "\n",
      "Epoch 10 Loss 0.0056\n",
      "Time take for 1 epoch 5.593072175979614 sec\n",
      "\n",
      "Epoch 10 Loss 0.0076\n",
      "Time take for 1 epoch 7.671796083450317 sec\n",
      "\n",
      "Epoch 10 Loss 0.0095\n",
      "Time take for 1 epoch 9.568428993225098 sec\n",
      "\n",
      "Epoch 10 Loss 0.0116\n",
      "Time take for 1 epoch 11.452003002166748 sec\n",
      "\n",
      "Epoch 10 Loss 0.0134\n",
      "Time take for 1 epoch 13.236100196838379 sec\n",
      "\n",
      "Epoch 10 Loss 0.0152\n",
      "Time take for 1 epoch 15.170241117477417 sec\n",
      "\n",
      "Epoch 10 Loss 0.0170\n",
      "Time take for 1 epoch 17.126885175704956 sec\n",
      "\n",
      "Epoch 10 Loss 0.0189\n",
      "Time take for 1 epoch 18.96233105659485 sec\n",
      "\n",
      "Epoch 10 Loss 0.0210\n",
      "Time take for 1 epoch 20.916383981704712 sec\n",
      "\n",
      "Epoch 10 Loss 0.0228\n",
      "Time take for 1 epoch 22.838688135147095 sec\n",
      "\n",
      "Epoch 10 Loss 0.0248\n",
      "Time take for 1 epoch 24.75947904586792 sec\n",
      "\n",
      "Epoch 10 Loss 0.0267\n",
      "Time take for 1 epoch 26.555840969085693 sec\n",
      "\n",
      "Epoch 10 Loss 0.0285\n",
      "Time take for 1 epoch 28.556149005889893 sec\n",
      "\n",
      "Epoch 10 Loss 0.0305\n",
      "Time take for 1 epoch 30.3138530254364 sec\n",
      "\n",
      "Epoch 10 Loss 0.0326\n",
      "Time take for 1 epoch 32.30906295776367 sec\n",
      "\n",
      "Epoch 10 Loss 0.0346\n",
      "Time take for 1 epoch 34.256500005722046 sec\n",
      "\n",
      "Epoch 10 Loss 0.0365\n",
      "Time take for 1 epoch 36.21484303474426 sec\n",
      "\n",
      "Epoch 10 Loss 0.0385\n",
      "Time take for 1 epoch 38.69829201698303 sec\n",
      "\n",
      "Epoch 10 Loss 0.0406\n",
      "Time take for 1 epoch 40.7311429977417 sec\n",
      "\n",
      "Epoch 10 Loss 0.0428\n",
      "Time take for 1 epoch 42.64858102798462 sec\n",
      "\n",
      "Epoch 10 Loss 0.0449\n",
      "Time take for 1 epoch 44.45005702972412 sec\n",
      "\n",
      "Epoch 10 Loss 0.0467\n",
      "Time take for 1 epoch 46.19303512573242 sec\n",
      "\n",
      "Epoch 10 Loss 0.0488\n",
      "Time take for 1 epoch 47.941128969192505 sec\n",
      "\n",
      "Epoch 10 Loss 0.0509\n",
      "Time take for 1 epoch 50.11789584159851 sec\n",
      "\n",
      "Epoch 10 Loss 0.0529\n",
      "Time take for 1 epoch 52.61161398887634 sec\n",
      "\n",
      "Epoch 10 Loss 0.0548\n",
      "Time take for 1 epoch 55.02444505691528 sec\n",
      "\n",
      "Epoch 10 Loss 0.0569\n",
      "Time take for 1 epoch 57.383557081222534 sec\n",
      "\n",
      "Epoch 10 Loss 0.0591\n",
      "Time take for 1 epoch 59.70407819747925 sec\n",
      "\n",
      "Epoch 10 Loss 0.0612\n",
      "Time take for 1 epoch 62.08844804763794 sec\n",
      "\n",
      "Epoch 10 Loss 0.0633\n",
      "Time take for 1 epoch 64.4532060623169 sec\n",
      "\n",
      "Epoch 10 Loss 0.0652\n",
      "Time take for 1 epoch 66.82165908813477 sec\n",
      "\n",
      "Epoch 10 Loss 0.0673\n",
      "Time take for 1 epoch 69.01142191886902 sec\n",
      "\n",
      "Epoch 10 Loss 0.0694\n",
      "Time take for 1 epoch 71.06753706932068 sec\n",
      "\n",
      "Epoch 10 Loss 0.0715\n",
      "Time take for 1 epoch 73.02173805236816 sec\n",
      "\n",
      "Epoch 10 Loss 0.0736\n",
      "Time take for 1 epoch 75.11025905609131 sec\n",
      "\n",
      "Epoch 10 Loss 0.0753\n",
      "Time take for 1 epoch 77.12711501121521 sec\n",
      "\n",
      "Epoch 10 Loss 0.0771\n",
      "Time take for 1 epoch 79.22161316871643 sec\n",
      "\n",
      "Epoch 10 Loss 0.0789\n",
      "Time take for 1 epoch 81.24798512458801 sec\n",
      "\n",
      "Epoch 10 Loss 0.0806\n",
      "Time take for 1 epoch 83.27833914756775 sec\n",
      "\n",
      "Epoch 10 Loss 0.0823\n",
      "Time take for 1 epoch 85.34524703025818 sec\n",
      "\n",
      "Epoch 10 Loss 0.0839\n",
      "Time take for 1 epoch 87.46547198295593 sec\n",
      "\n",
      "Epoch 10 Loss 0.0855\n",
      "Time take for 1 epoch 89.66752195358276 sec\n",
      "\n",
      "Epoch 10 Loss 0.0872\n",
      "Time take for 1 epoch 91.66995620727539 sec\n",
      "\n",
      "Epoch 10 Loss 0.0891\n",
      "Time take for 1 epoch 93.5192482471466 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = len(input_tensor) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    encoding_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, encoding_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "        \n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "        \n",
    "        print('Time take for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_sentence):\n",
    "    attention_matrix = np.zeros((max_length_output, max_length_input))\n",
    "    input_sentence = preprocess_sentence(input_sentence)\n",
    "    \n",
    "    inputs = [input_tokenizer.word_index[token] for token in input_sentence.split(' ')]\n",
    "    inputs = keras.preprocessing.sequence.pad_sequences(\n",
    "        [inputs], maxlen = max_length_input, padding = 'post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    results = ''\n",
    "#     encoding_hidden = encoder.initialize_hidden_state()\n",
    "    encoding_hidden = tf.zeros((1, units))\n",
    "    \n",
    "    encoding_outputs, encoding_hidden = encoder(inputs, encoding_hidden)\n",
    "    decoding_hidden = encoding_hidden\n",
    "    \n",
    "    # eg: <start> -> A\n",
    "    # A -> B -> C -> D\n",
    "    \n",
    "    decoding_input = tf.expand_dims([output_tokenizer.word_index['<start>']], 0)\n",
    "    for t in range(max_length_output):\n",
    "        predictions, decoding_hidden, attention_weights = decoder(decoding_input,\n",
    "                                                                  decoding_hidden,\n",
    "                                                                  encoding_outputs)\n",
    "        # attention_weights.shape: (batch_size, input_length, 1)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
    "        attention_matrix[t] = attention_weights.numpy()\n",
    "        \n",
    "        # predictions.shape: (batch_size, vocab_size)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        results += output_tokenizer.index_word[predicted_id] + ' '\n",
    "        \n",
    "        if output_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return results, input_sentence, attention_matrix\n",
    "        \n",
    "        decoding_input = tf.expand_dims([predicted_id], 0)\n",
    "    return results, input_sentence, attention_matrix\n",
    "\n",
    "def plot_attention(attention_matrix, input_sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention_matrix, cmap='viridis')\n",
    "\n",
    "    font_dict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + input_sentence, fontdict = font_dict, rotation = 90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict = font_dict,)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def translate(input_sentence):\n",
    "    results, input_sentence, attention_matrix = evaluate(input_sentence)\n",
    "    \n",
    "    print('Input: %s' %(input_sentence))\n",
    "    print('Predicted translation: %s' %(results))\n",
    "    \n",
    "    attention_matrix = attention_matrix[:len(results.split(' ')),\n",
    "                                       :len(input_sentence.split(' '))]\n",
    "    plot_attention(attention_matrix, input_sentence.split(' '), results.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Predicted translation: he s not in . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'Hace mucho frío aquí.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_2.0_env",
   "language": "python",
   "name": "tensorflow_2.0_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
